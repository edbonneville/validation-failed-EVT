---
title: "Validation failed EVT"
author: "E. F. Bonneville"
date: "`r Sys.setenv(LANG = 'en_US.UTF-8'); format(Sys.Date(), '%d %B %Y')`"
output: 
  html_document:
    keep_md: true
    df_print: kable
    toc: yes
    toc_float: 
      smooth_scroll: false
editor_options: 
  chunk_output_type: console
params:
  interactive_tables: FALSE
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = here::here())
knitr::opts_chunk$set(
  echo = FALSE, 
  out.width = "100%", 
  warning = FALSE, 
  message = FALSE
)

# Load libraries just for analysis rmd
library(ggplot2)
library(targets)
library(tidyverse)
library(tableone)
library(rms)
library(glmnet)
library(DT)
library(broom.helpers)
library(broom)
library(knitr)
```

```{r objs}
# Source functions
source("data-raw/prepare_raw_data.R")
source("R/imputation-helpers.R")
source("R/model-validation-helpers.R")
source("R/calplot-MI.R")

# Set contrasts for ordered factors
options(contrasts = rep("contr.treatment", 2))

# Set global setting
theme_set(theme_minimal(base_size = 14))

tar_load(
  c(
    dat_combined,
    dat_to_impute,
    imps_all,
    candidate_predictors,
    model_formula,
    analysis_settings,
    validation_dev_lambdamin,
    validation_comb_lambdamin
  )
)

# For later use
response_var <- all.vars(stats::update(model_formula, . ~ 1))
```

## Descriptives

### Table 1

For continuous variables:

```{r table-one-contin}
# ?tar_mermaid()

# Gives you everything for tables 1 and 2 (warnings due to variables not in validation, ignore!)
table_one <- select(.data = tar_read(develop_n887_raw), c("StudySubjectID", "M_mrs_rev")) |> 
  bind_rows(select(.data = tar_read(valid_n1111_raw), c("StudySubjectID", "M_mrs_rev"))) |> 
  rename("M_mrs_rev_orig" = "M_mrs_rev") |> 
  left_join(dat_combined) %>%  
  select(-all_of(c("sidescored", "AngleCcaIca", "ICAEAtherosclerosis", "StudySubjectID", "M_mrs_rev"))) %>%
  # Here we dichotomise purely for the table, does not affect original data
  mutate(
    ICA_nr_90orLarger_geq1 = factor(ifelse(ICA_nr_90orLarger >= 1, 1, 0)),
    ICA_nr_90orLarger_geq2 = factor(ifelse(ICA_nr_90orLarger >= 2, 1, 0)),
    ICAE_NASCET_99 = factor(ifelse(ICAE_NASCET_Degree < 99, 0,1)),
    InnCca_90orLarger_geq1 = factor(ifelse(InnCca_nr_90orLarger >= 1, 1, 0)),
    InnCca_90orLarger_geq2 = factor(ifelse(InnCca_nr_90orLarger >= 2, 1, 0)),
    ICAI_stenosis50 = factor(ifelse(ICAIAtherosclerosis == "Yes, >=50% stenosis", 1, 0)),
    AngleAaInn_OR_AaCca_dich45 = factor(ifelse(AngleAaInn_OR_AaCca > 45, 0, 1)),
    Mc_pretici_short = fct_collapse(
      Mc_pretici_short, 
      "<1" = c("0"),
      ">=1" = c("1", "2A", "2B, 2C or 3")
    ),
    M_collaterals = fct_collapse(
      M_collaterals,
      "<50" = c("absent collaterals", "filling <50% of occluded area"),
      ">=50" = c(">50% but less <100%", "100% of occluded area")
    ),
    M_ASPECTS_BL = factor(
      ifelse(M_ASPECTS_BL > 7, 0, 1),
      levels = c(0, 1),
      labels = c("8-10", "<=7")
    ),
    M_CBS_BL = factor(
      ifelse(M_CBS_BL > 7, 0, 1),
      levels = c(0, 1),
      labels = c("8-10", "<=7")
    ),
    M_posttici_c = fct_collapse(
      M_posttici_c,
      "2B-3" = c("2B", "2C", "3"),
      "0-2A" = c("0", "1", "2A")
    ),
    M_mrs_rev_orig = factor(
      ifelse(M_mrs_rev_orig >= 4, 0, 1),
      levels = c(0, 1),
      labels = c("0-2", "3-6")
    )
  ) %>% 
  CreateTableOne(data = ., strata = "dataset")

# Summaries of all continuous variables
invisible(
  capture.output(
    tab_contin <- print(
      x = table_one$ContTable, 
      nonnormal = TRUE, 
      noSpaces = TRUE
    )
  )
)

#https://stackoverflow.com/questions/50039186/add-download-buttons-in-dtrenderdatatable
if (params$interactive_tables) {
  DT::datatable(
    tab_contin,
    extensions = 'Buttons',
    options = list(
      dom = 'Blfrtip', 
      buttons = c('csv', 'excel', 'pdf'),
      lengthMenu = list(c(10, 25, 50, -1), c(10, 25, 50, "All"))
    )
  )
} else kableone(table_one$ContTable)

```

For categorical variables:

```{r table-one-categ}
# Summaries of all categorical variables
invisible(
  capture.output(
    tab_cat <- print(
      x = table_one$CatTable, 
      nonnormal = TRUE, 
      noSpaces = TRUE
    )
  )
)

if (params$interactive_tables) {
  DT::datatable(
    tab_cat,
    extensions = 'Buttons',
    options = list(
      dom = 'Blfrtip', 
      buttons = c('csv', 'excel', 'pdf'),
      lengthMenu = list(c(10, 25, 50, -1), c(10, 25, 50, "All"))
    )
  )
} else kableone(table_one$CatTable)
```

### Missing data summaries 


```{r cca-check}
# Seperate datasets
dat_develop <- subset(x = dat_to_impute, select = -dataset, subset = (dataset == "develop"))

dat_valid <- subset(x = dat_to_impute, select = -dataset, subset = (dataset == "valid"))

# Development
prop_cca_dev <- mean(complete.cases(dat_develop[, c("Mc_FailedFemoralApproach", candidate_predictors)]))

# Validation
prop_cca_valid <- mean(complete.cases(dat_valid[, c("Mc_FailedFemoralApproach", candidate_predictors)]))

# Combined
prop_cca_comb <- mean(complete.cases(dat_to_impute[, c("Mc_FailedFemoralApproach", candidate_predictors)]))
```

Complete-case analysis (using predictors in the prediction model) results in:

- `r paste0(round(prop_cca_dev * 100, 2), " %")` complete cases in the development data
- `r paste0(round(prop_cca_valid * 100, 2), " %")` complete cases in the validation data
- `r paste0(round(prop_cca_comb * 100, 2), " %")` complete cases in the combined data

The next plots look at the combinations of variables missings and their frequencies in the development and validation sets respectively:

```{r miss-dat-summ}
# Visualise missings beforehand
naniar::gg_miss_var(dat_to_impute, facet = dataset, show_pct = TRUE)
#naniar::gg_miss_upset(dat_develop, nsets = 10, )
#naniar::gg_miss_upset(dat_valid, nsets = 10)

df_missings <- dat_to_impute |> 
  group_by(dataset) |> 
  naniar::miss_var_summary() |> 
  mutate(across(where(is.numeric), ~ round(.x, 2L))) 

if (params$interactive_tables)  {
  DT::datatable(
    df_missings,
    extensions = 'Buttons',
    options = list(
      dom = 'Blfrtip', 
      buttons = c('csv', 'excel', 'pdf'),
      lengthMenu = list(c(10, 25, 50, -1), c(10, 25, 50, "All"))
    )
  )  
} else kable(df_missings)
```

<!-- (Naniar/JointAI summaries/VIM) -->

## Internal validation development set

Coefficients of the (penalized, with Ridge) prediction model built using the development data:

```{r dev-coefs}
# Development imputation
imps_develop <- imps_all %>% 
  filter(imps_label == "imps_assess" & dataset == "develop")

# Development model
mod_develop <- validation_dev_lambdamin$model_fit
mod_coefs <- as.matrix(coef(mod_develop))
colnames(mod_coefs) <- "Coefficients"

# Print penalised model coefficients
#knitr::kable(data.frame(mod_coefs))

if (params$interactive_tables)  {

  DT::datatable(
    data.frame(round(mod_coefs, digits = 3)),
    extensions = 'Buttons',
    options = list(
      dom = 'Blfrtip', 
      buttons = c('csv', 'excel', 'pdf'),
      lengthMenu = list(c(10, 25, 50, -1), c(10, 25, 50, "All"))
    )
  )
} else kable(data.frame(round(mod_coefs, digits = 3)))
```

Bootstrap-based internal validation, optimism-corrected performance of this model:

```{r valid-dev}
# Optimism corrected
validation_dev_lambdamin$validation_summary %>% 
  filter(measure %in% c("auc", "intercept", "slope")) %>% 
  group_by(measure) %>% 
  summarise(value = paste0(corrected, " [", lower_corrected, ";", upper_corrected, "]")) %>%
  knitr::kable()
```

Calibration plot, reflecting apparent performance:

```{r dev-calib}
calplot_MI(
  imps_long = imps_all %>% 
    filter(imps_label == "imps_assess" & dataset == "develop"),
  model = validation_dev_lambdamin$model_fit,
  impdat_ind = ".imp",
  model_formula = model_formula,
  height_hist = 0.2,
  startpos_hist = -0.25,
  knots = 5,
  n_bins = 100,
  xlim = c(0, 0.85),
  ylim = c(-0.25, 1)
) +
  theme_minimal()
```

## External validation

```{r ext-validation}
imps_valid <- imps_all %>% 
  filter(imps_label == "imps_assess" & dataset == "valid")

X_valid <- subset(x = stats::model.matrix(model_formula, data = imps_valid), select = -`(Intercept)`)

# Compute linear predictor
lp_valid <- drop(predict(mod_develop, newx = X_valid))
df_calplot_valid <- cbind.data.frame(lp_valid, imps_valid)

# Pool AUC
auc_df <- split(df_calplot_valid, ~ .imp) %>%
  map(.f = ~ {
    auc_obj <- pROC::auc(Mc_FailedFemoralApproach ~ lp_valid, data = .x)
    cbind.data.frame("auc" = as.numeric(auc_obj), "var_auc" = pROC::var(auc_obj))
  }) %>%
  bind_rows(.id = ".imp") 

auc_ext <- psfmi::pool_auc(
  est_auc = auc_df$auc,
  est_se = sqrt(auc_df$var_auc),
  nimp = max(as.numeric(auc_df$.imp))
)

# Get cal slope and intercept
cal_slope_ext <- split(df_calplot_valid, ~ .imp) |> 
  map(.f = ~ glm(Mc_FailedFemoralApproach ~ lp_valid, family = binomial, data = .x)) |> 
  mice::pool() |> 
  summary(conf.int = TRUE)

cal_int_ext <- split(df_calplot_valid, ~ .imp) |> 
  map(.f = ~ glm(Mc_FailedFemoralApproach ~ offset(lp_valid), family = binomial, data = .x)) |> 
  mice::pool() |> 
  summary(conf.int = TRUE)

int_ext <- with(
  cal_int_ext, 
  paste0(
    round(estimate, 3), " [",
    round(`2.5 %`, 3), ", ",
    round(`97.5 %`, 3), "]"
  )
)

slope_ext <- with(
  cal_slope_ext[2, ], 
  paste0(
    round(estimate, 3), " [",
    round(`2.5 %`, 3), ", ",
    round(`97.5 %`, 3), "]"
  )
)

auc_ext_bis <- with(
  as.data.frame(auc_ext),
  paste0(
    round(`C-statistic`, 3), " [",
    round(`95% Low`, 3), ", ",
    round(`95% Up`, 3), "]"
  )
)

rbind.data.frame(
  cbind.data.frame("measure" = "auc", "value" = auc_ext_bis),
  cbind.data.frame("measure" = "intercept", "value" = int_ext),
  cbind.data.frame("measure" = "slope", "value" = slope_ext)
)

# Overestimation of risks
calplot_ext <- calplot_MI(
  imps_long = imps_valid,
  model = validation_dev_lambdamin$model_fit,
  impdat_ind = ".imp",
  model_formula = model_formula,
  height_hist = 0.2,
  startpos_hist = -0.25,
  knots = 5,
  n_bins = 200,
  xlim = c(0, 0.7),
  ylim = c(-0.25, 1)
) +
  theme_minimal() + 
  labs(x = "Predicted probability", y = "Observed proportion")

calplot_ext

ggsave(
  calplot_ext,
  filename = "analysis/figures/calibration-plot-ext.png",
  width = 8,
  height = 5,
  units = "in",
  dpi = 300, 
  bg = "white"
)
```

At external validation, the calibration plot shows that there is quite some overestimation of risks in the validation set.

## Model update: combined cohort re-estimation

### Interaction-by-cohort (unpenalized)

See [Balmaña, Stockwell, Steyerberg et al. (2006)](https://jamanetwork.com/journals/jama/article-abstract/203427). Here, we use standard (unpenalized) logistics regression to test for differences between the development and validation set in terms of multivariable model coefficients.

The log odds table of coefficient, and the p-values for the interaction:

```{r interaction-by-cohort}
imps_combined <- imps_all %>% 
  filter(imps_label == "imps_combined")

inter_model <- split(imps_combined, ~ .imp) |> 
  map(.f = ~ glm(update(model_formula, . ~ . * dataset), family = binomial, data = .x)) |> 
  mice::pool() |> 
  broom::tidy() 

summ_inter <- cbind.data.frame(
  inter_model |>  
    filter(str_detect(term, pattern = "valid$", negate = TRUE)) |> 
    select(term),
  "development" = inter_model |>  
    filter(str_detect(term, pattern = "valid$", negate = TRUE)) |> 
    pull(estimate),
  "interaction" = inter_model |>  
    filter(str_detect(term, pattern = "valid$")) |> 
    pull(estimate),
  "pval_interaction" = inter_model |>  
    filter(str_detect(term, pattern = "valid$")) |> 
    pull(p.value)
) |> 
  mutate(
    validation = development + interaction
  ) |> 
  mutate(across(where(is.numeric), ~ round(.x, 3)))

if (params$interactive_tables) {
  DT::datatable(
    summ_inter,
    extensions = 'Buttons',
    options = list(
      dom = 'Blfrtip', 
      buttons = c('csv', 'excel', 'pdf'),
      lengthMenu = list(c(10, 25, 50, -1), c(10, 25, 50, "All"))
    )
  )
} else kable(summ_inter)
```

In the format of Table 4 from the article mentioned above (with odds ratios):

```{r inter-steyerberg}
# Second attempt in Ewout format 
dev_unpen <- split(imps_combined |> 
                     filter(dataset == "develop"), ~ .imp) |> 
  map(.f = ~ glm(model_formula, family = binomial, data = .x)) |> 
  mice::pool() |> 
  #broom::tidy(exponentiate = TRUE, conf.int = TRUE) |> 
  tidy(exponentiate = TRUE, conf.int = TRUE) |> 
  #tidy_add_reference_rows(no_reference_row = all_categorical())
  mutate(
    "Development" = paste0(
      round(estimate, 3), " [",
      round(conf.low, 3), ", ",
      round(conf.high, 3), "]"
    )
  ) |> 
  select(term, Development)

valid_unpen <- split(imps_combined |> 
                     filter(dataset == "valid"), ~ .imp) |> 
  map(.f = ~ glm(model_formula, family = binomial, data = .x)) |> 
  mice::pool() |> 
  #broom::tidy(exponentiate = TRUE, conf.int = TRUE) |> 
  tidy(exponentiate = TRUE, conf.int = TRUE) |> 
  #tidy_add_reference_rows(no_reference_row = all_categorical())
  mutate(
    "Validation" = paste0(
      round(estimate, 3), " [",
      round(conf.low, 3), ", ",
      round(conf.high, 3), "]"
    )
  ) |> 
  select(term, Validation)

comb_unpen <- split(imps_combined, ~ .imp) |> 
  map(.f = ~ glm(model_formula, family = binomial, data = .x)) |> 
  mice::pool() |> 
  #broom::tidy(exponentiate = TRUE, conf.int = TRUE) |> 
  tidy(exponentiate = TRUE, conf.int = TRUE) |> 
  #tidy_add_reference_rows(no_reference_row = all_categorical())
  mutate(
    "Combined" = paste0(
      round(estimate, 3), " [",
      round(conf.low, 3), ", ",
      round(conf.high, 3), "]"
    )
  ) |> 
  select(term, Combined)

df_unpen <- cbind.data.frame(
  "Term" = dev_unpen$term, 
  "Development cohort" = dev_unpen$Development, 
  "Validation cohort" = valid_unpen$Validation,
  "Combined cohort" = comb_unpen$Combined
) 

if (params$interactive_tables) {
  DT::datatable(
    df_unpen,
    extensions = 'Buttons',
    options = list(
      dom = 'Blfrtip', 
      buttons = c('csv', 'excel', 'pdf'),
      lengthMenu = list(c(10, 25, 50, -1), c(10, 25, 50, "All"))
    )
  )
} else kable(df_unpen)
```

### Internal validation combined cohort

```{r combined-validation}
mod_combined <- validation_comb_lambdamin$model_fit
#validation_comb_lambdamin$model_fit$lambda

mod_coefs_comb <- as.matrix(coef(mod_combined))
colnames(mod_coefs_comb) <- "Coefficients"

# Print penalised model coefficients
if (params$interactive_tables) {
  DT::datatable(
    data.frame(round(mod_coefs_comb, digits = 3)),
    extensions = 'Buttons',
    options = list(
      dom = 'Blfrtip', 
      buttons = c('csv', 'excel', 'pdf'),
      lengthMenu = list(c(10, 25, 50, -1), c(10, 25, 50, "All"))
    )
  )
} else kable(data.frame(round(mod_coefs_comb, digits = 3)))



# Optimism corrected
validation_comb_lambdamin$validation_summary %>% 
  filter(measure %in% c("auc", "intercept", "slope")) %>% 
  group_by(measure) %>% 
  summarise(value = paste0(corrected, " [", lower_corrected, ";", upper_corrected, "]")) %>%
  knitr::kable()

calplot_combined <- calplot_MI(
  imps_long = imps_combined,
  model = validation_comb_lambdamin$model_fit,
  impdat_ind = ".imp",
  model_formula = model_formula,
  height_hist = 0.2,
  startpos_hist = -0.25,
  knots = 5,
  n_bins = 200,
  xlim = c(0, 0.55),
  ylim = c(-0.25, 0.8)
) +
  theme_minimal() +
  labs(x = "Predicted probability", y = "Observed proportion")

calplot_combined

ggsave(
  calplot_combined,
  filename = "analysis/figures/calibration-plot-combined.png",
  width = 8,
  height = 5,
  units = "in",
  dpi = 300, bg = "white"
)
```

### Nomogram

```{r nomogram}
# Make data for nomogram labels
dat_nomogram <- imps_combined %>% 
  select(all_of(c(response_var, candidate_predictors))) %>% 
  mutate(
    InnCca_nr_90orLarger = factor(
      InnCca_nr_90orLarger,
      levels = levels(InnCca_nr_90orLarger),
      labels = c("0", ">=1")
    ),
    ICAE_NASCET_Degree = factor(
      ICAE_NASCET_Degree,
      levels = levels(ICAE_NASCET_Degree),
      labels = c("<99", ">=99")
    )
  )

dd <<- datadist(dat_nomogram)
options(datadist = "dd")

# Set variable labels
Hmisc::label(dat_nomogram$M_age) <- "Age (per decade)"
Hmisc::label(dat_nomogram$M_prev_ht) <- "Hypertension"
Hmisc::label(dat_nomogram$ArchElongation) <- "Aortic Arch Elongation"
Hmisc::label(dat_nomogram$AorticVariant) <- "Aortic variants"
Hmisc::label(dat_nomogram$InnCca_nr_90orLarger) <- "Tortuosity IA and CCA"
Hmisc::label(dat_nomogram$AngleFollowingBifurcation) <- "Tortuosity cervical ICA"
Hmisc::label(dat_nomogram$ICAE_NASCET_Degree) <- "ICA stenosis"

# Edit some labels
model_formula_reorder <- Mc_FailedFemoralApproach ~ M_age + M_prev_ht + ArchElongation +
  AorticVariant + InnCca_nr_90orLarger + AngleFollowingBifurcation + ICAE_NASCET_Degree

# Annoying stuff to reorder
mod_lrm_reorder <- lrm(model_formula_reorder, data = dat_nomogram)
mod_lrm <- lrm(model_formula, data = dat_nomogram)
mod_lrm$coefficients <- setNames(unname(drop(mod_coefs_comb)), names(mod_lrm$coefficients))
mod_lrm_reorder$coefficients <- mod_lrm$coefficients[match(
  names(mod_lrm_reorder$coefficients), names(mod_lrm$coefficients)
)]

par(mar = c(2,2,1,1))
nom <- nomogram(
  fit = mod_lrm_reorder, 
  fun = plogis, 
  maxscale = 10, 
  funlabel = "Predicted probability", 
  lp = FALSE, 
  fun.at = c(.01, .05, seq(.1, 1, by = .1))
)

# Adjust levels
nom$`Tortuosity cervical ICA`$AngleFollowingBifurcation <- c("<90°", ">=90°") #°
nom$`Tortuosity IA and CCA`$InnCca_nr_90orLarger <- c("<90°", ">=90°") #°
nom$`ICA stenosis`$ICAE_NASCET_Degree <- c("<99%", ">=99%") #°
nom$`Aortic variants`$AorticVariant <- c("Normal", "Common origin\nIA-CCA", "Bovine arch") #°
nom$`Aortic Arch Elongation`$ArchElongation <- c("No elongation", "Mild to moderate", "Severe") #°

plot(nom)

# Printing points
print(nom, dec = 2L)

# Get max probability in combined dataset?
```


```{r nom-export, results='hide'}
png(
  filename = "analysis/figures/nomogram-combined.png",
  width = 8,
  height = 11,
  units = "in",
  res = 300
)
plot(nom)
dev.off()

# Predict max prob
newpat <- dat_nomogram[1, ]
newpat$ArchElongation <- "III"
newpat$AngleFollowingBifurcation <- "Yes, angle >=90 degrees"
newpat$InnCca_nr_90orLarger <- ">=1"
newpat$ICAE_NASCET_Degree <- ">=99"
newpat$M_age <- 10
newpat

predict(mod_lrm, type = "fitted", newdata = newpat)
dat_nomogram[which.max(predict(mod_lrm, type = "fitted", newdata = dat_nomogram)), ]
predict(mod_lrm, type = "fitted", newdata = dat_nomogram) |>  max()
```
